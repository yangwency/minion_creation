{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import some basic packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import urllib\n",
    "import numpy as np\n",
    "import zipfile\n",
    "import os\n",
    "from scipy.io import wavfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### urllib is to download some files and set up the path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download(path):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    path : str\n",
    "        Url to download\n",
    "    Returns\n",
    "    -------\n",
    "    path : str\n",
    "        Location of downloaded file.\n",
    "    \"\"\"\n",
    "    import os\n",
    "    from six.moves import urllib\n",
    "\n",
    "    fname = path.split('/')[-1]\n",
    "    if os.path.exists(fname):\n",
    "        return fname\n",
    "\n",
    "    print('Downloading ' + path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up filepath and define the storage of files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def progress(count, block_size, total_size):\n",
    "        if count % 20 == 0:\n",
    "            print('Downloaded %02.02f/%02.02f MB' % (\n",
    "                count * block_size / 1024.0 / 1024.0,\n",
    "                total_size / 1024.0 / 1024.0), end='\\r')\n",
    "\n",
    "    filepath, _ = urllib.request.urlretrieve(\n",
    "        path, filename=fname, reporthook=progress)\n",
    "    return filepath"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download and extract tar files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_and_extract_tar(path, dst):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    path : str\n",
    "        Url to tar file to download.\n",
    "    dst : str\n",
    "        Location to save tar file contents.\n",
    "    \"\"\"\n",
    "    import tarfile\n",
    "    filepath = download(path)\n",
    "    if not os.path.exists(dst):\n",
    "        os.makedirs(dst)\n",
    "        tarfile.open(filepath, 'r:gz').extractall(dst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download and extract zip files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_and_extract_zip(path, dst):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    path : str\n",
    "        Url to zip file to download.\n",
    "    dst : str\n",
    "        Location to save zip file contents.\n",
    "    \"\"\"\n",
    "    import zipfile\n",
    "    filepath = download(path)\n",
    "    if not os.path.exists(dst):\n",
    "        os.makedirs(dst)\n",
    "        zf = zipfile.ZipFile(file=filepath)\n",
    "        zf.extractall(dst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use scipy.io.wavfile to add the audiofile at the filename."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_audio(filename, b_normalize=True):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    filename : str\n",
    "        File to load.\n",
    "    b_normalize : bool, optional\n",
    "        Normalize to the maximum value.\n",
    "    \"\"\"\n",
    "    sr, s = wavfile.read(filename)\n",
    "    if b_normalize:\n",
    "        s = s.astype(np.float32)\n",
    "        s = (s / np.max(np.abs(s)))\n",
    "        s -= np.mean(s)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use an input tensor and add uniform masking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corrupt(x):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    x : Tensor/Placeholder\n",
    "        Input to corrupt.\n",
    "    Returns\n",
    "    -------\n",
    "    x_corrupted : Tensor\n",
    "        50 pct of values corrupted.\n",
    "    \"\"\"\n",
    "    return tf.multiply(x, tf.cast(tf.random_uniform(shape=tf.shape(x),\n",
    "                                               minval=0,\n",
    "                                               maxval=2,\n",
    "                                               dtype=tf.int32), tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Insert values between the left and right edges of all samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interp(l, r, n_samples):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    l : np.ndarray\n",
    "        Left edge\n",
    "    r : np.ndarray\n",
    "        Right edge\n",
    "    n_samples : int\n",
    "        Number of samples\n",
    "    Returns\n",
    "    -------\n",
    "    arr : np.ndarray\n",
    "        Inteporalted array\n",
    "    \"\"\"\n",
    "    return np.array([\n",
    "        l + step_i / (n_samples - 1) * (r - l)\n",
    "        for step_i in range(n_samples)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a two-dimensional manifold from samples(n_samples * n_samples) and there are 4 corners in each sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_latent_manifold(corners, n_samples):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    corners : list of np.ndarray\n",
    "        The four corners to intepolate.\n",
    "    n_samples : int\n",
    "        Number of samples to use in interpolation.\n",
    "    Returns\n",
    "    -------\n",
    "    arr : np.ndarray\n",
    "        Stacked array of all 2D interpolated samples\n",
    "    \"\"\"\n",
    "    left = interp(corners[0], corners[1], n_samples)\n",
    "    right = interp(corners[2], corners[3], n_samples)\n",
    "\n",
    "    embedding = []\n",
    "    for row_i in range(n_samples):\n",
    "        embedding.append(interp(left[row_i], right[row_i], n_samples))\n",
    "    return np.vstack(embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Turn all sample patterns into squares."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imcrop_tosquare(img):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    img : np.ndarray\n",
    "        Input image to crop, assumed at least 2d.\n",
    "    Returns\n",
    "    -------\n",
    "    crop : np.ndarray\n",
    "        Cropped image.\n",
    "    \"\"\"\n",
    "    size = np.min(img.shape[:2])\n",
    "    extra = img.shape[:2] - size\n",
    "    crop = img\n",
    "    for i in np.flatnonzero(extra):\n",
    "        crop = np.take(crop, extra[i] // 2 + np.r_[:size], axis=i)\n",
    "    return crop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cut a montage image into n times h times w images(n means number, h means height, w means width). And then performs the opposite of the montage function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def slice_montage(montage, img_h, img_w, n_imgs):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    montage : np.ndarray\n",
    "        Montage image to slice.\n",
    "    img_h : int\n",
    "        Height of sliced image\n",
    "    img_w : int\n",
    "        Width of sliced image\n",
    "    n_imgs : int\n",
    "        Number of images to slice\n",
    "    Returns\n",
    "    -------\n",
    "    sliced : np.ndarray\n",
    "        Sliced images as 4d array.\n",
    "    \"\"\"\n",
    "    sliced_ds = []\n",
    "    for i in range(int(np.sqrt(n_imgs))):\n",
    "        for j in range(int(np.sqrt(n_imgs))):\n",
    "            sliced_ds.append(montage[\n",
    "                1 + i + i * img_h:1 + i + (i + 1) * img_h,\n",
    "                1 + j + j * img_w:1 + j + (j + 1) * img_w])\n",
    "    return np.array(sliced_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Draw all the images as montages separated by 1 pixel border and  save the file to the target specified by `saveto`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def montage(images, saveto='montage.png'):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    images : numpy.ndarray\n",
    "        Input array to create montage of.  Array should be:\n",
    "        batch x height x width x channels.\n",
    "    saveto : str\n",
    "        Location to save the resulting montage image.\n",
    "    Returns\n",
    "    -------\n",
    "    m : numpy.ndarray\n",
    "        Montage image.\n",
    "    \"\"\"\n",
    "    if isinstance(images, list):\n",
    "        images = np.array(images)\n",
    "    img_h = images.shape[1]\n",
    "    img_w = images.shape[2]\n",
    "    n_plots = int(np.ceil(np.sqrt(images.shape[0])))\n",
    "    if len(images.shape) == 4 and images.shape[3] == 3:\n",
    "        m = np.ones(\n",
    "            (images.shape[1] * n_plots + n_plots + 1,\n",
    "             images.shape[2] * n_plots + n_plots + 1, 3)) * 0.5\n",
    "    else:\n",
    "        m = np.ones(\n",
    "            (images.shape[1] * n_plots + n_plots + 1,\n",
    "             images.shape[2] * n_plots + n_plots + 1)) * 0.5\n",
    "    for i in range(n_plots):\n",
    "        for j in range(n_plots):\n",
    "            this_filter = i * n_plots + j\n",
    "            if this_filter < images.shape[0]:\n",
    "                this_img = images[this_filter]\n",
    "                m[1 + i + i * img_h:1 + i + (i + 1) * img_h,\n",
    "                  1 + j + j * img_w:1 + j + (j + 1) * img_w] = this_img\n",
    "    plt.imsave(arr=m, fname=saveto)\n",
    "    return m\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Draw all filters as montage images separated by 1 pixel border and the filter is n_input * n_output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def montage_filters(W):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    W : Tensor\n",
    "        Input tensor to create montage of.\n",
    "    Returns\n",
    "    -------\n",
    "    m : numpy.ndarray\n",
    "        Montage image.\n",
    "    \"\"\"\n",
    "    W = np.reshape(W, [W.shape[0], W.shape[1], 1, W.shape[2] * W.shape[3]])\n",
    "    n_plots = int(np.ceil(np.sqrt(W.shape[-1])))\n",
    "    m = np.ones(\n",
    "        (W.shape[0] * n_plots + n_plots + 1,\n",
    "         W.shape[1] * n_plots + n_plots + 1)) * 0.5\n",
    "    for i in range(n_plots):\n",
    "        for j in range(n_plots):\n",
    "            this_filter = i * n_plots + j\n",
    "            if this_filter < W.shape[-1]:\n",
    "                m[1 + i + i * W.shape[0]:1 + i + (i + 1) * W.shape[0],\n",
    "                  1 + j + j * W.shape[1]:1 + j + (j + 1) * W.shape[1]] = (\n",
    "                    np.squeeze(W[:, :, :, this_filter]))\n",
    "    return m\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the 100 images of the dataset and files will be placed in a directory 'img_align_celeba'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_celeb_files(dst='img_align_celeba', max_images=100):\n",
    "    \"\"\"\n",
    "    Returns\n",
    "    -------\n",
    "    files : list of strings\n",
    "        Locations to the first 100 images of the celeb net dataset.\n",
    "    \"\"\"\n",
    "    # Create a directory\n",
    "    if not os.path.exists(dst):\n",
    "        os.mkdir(dst)\n",
    "\n",
    "    # Now perform the following 100 times:\n",
    "    for img_i in range(1, max_images + 1):\n",
    "\n",
    "        # create a string using the current loop counter\n",
    "        f = '000%03d.jpg' % img_i\n",
    "\n",
    "        if not os.path.exists(os.path.join(dst, f)):\n",
    "\n",
    "            # and get the url with that string appended the end\n",
    "            url = 'https://s3.amazonaws.com/cadl/celeb-align/' + f\n",
    "\n",
    "            # We'll print this out to the console so we can see how far we've gone\n",
    "            print(url, end='\\r')\n",
    "\n",
    "            # And now download the url to a location inside our new directory\n",
    "            urllib.request.urlretrieve(url, os.path.join(dst, f))\n",
    "\n",
    "    files = [os.path.join(dst, file_i)\n",
    "             for file_i in os.listdir(dst)\n",
    "             if '.jpg' in file_i][:max_images]\n",
    "    return files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the first `max_images` image of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_celeb_imgs(max_images=100):\n",
    "    \"\"\"\n",
    "    Returns\n",
    "    -------\n",
    "    imgs : list of np.ndarray\n",
    "        List of the first 100 images from the celeb dataset\n",
    "    \"\"\"\n",
    "    return [plt.imread(f_i) for f_i in get_celeb_files(max_images=max_images)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define three parameters and compute a Gaussian Kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gauss(mean, stddev, ksize):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    mean : float\n",
    "        Mean of the Gaussian (e.g. 0.0).\n",
    "    stddev : float\n",
    "        Standard Deviation of the Gaussian (e.g. 1.0).\n",
    "    ksize : int\n",
    "        Size of kernel (e.g. 16).\n",
    "    Returns\n",
    "    -------\n",
    "    kernel : np.ndarray\n",
    "        Computed Gaussian Kernel using Tensorflow.\n",
    "    \"\"\"\n",
    "    g = tf.Graph()\n",
    "    with tf.Session(graph=g):\n",
    "        x = tf.linspace(-3.0, 3.0, ksize)\n",
    "        z = (tf.exp(tf.negative(tf.pow(x - mean, 2.0) /\n",
    "                           (2.0 * tf.pow(stddev, 2.0)))) *\n",
    "             (1.0 / (stddev * tf.sqrt(2.0 * 3.1415))))\n",
    "        return z.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define three parameters and compute a two-dimention Gaussian Kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gauss2d(mean, stddev, ksize):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    mean : float\n",
    "        Mean of the Gaussian (e.g. 0.0).\n",
    "    stddev : float\n",
    "        Standard Deviation of the Gaussian (e.g. 1.0).\n",
    "    ksize : int\n",
    "        Size of kernel (e.g. 16).\n",
    "    Returns\n",
    "    -------\n",
    "    kernel : np.ndarray\n",
    "        Computed 2D Gaussian Kernel using Tensorflow.\n",
    "    \"\"\"\n",
    "    z = gauss(mean, stddev, ksize)\n",
    "    g = tf.Graph()\n",
    "    with tf.Session(graph=g):\n",
    "        z_2d = tf.matmul(tf.reshape(z, [ksize, 1]), tf.reshape(z, [1, ksize]))\n",
    "        return z_2d.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolve a 4D image with a 4D kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convolve(img, kernel):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    img : np.ndarray\n",
    "        4-dimensional image shaped N x H x W x C\n",
    "    kernel : np.ndarray\n",
    "        4-dimensional image shape K_H, K_W, C_I, C_O corresponding to the\n",
    "        kernel's height and width, the number of input channels, and the\n",
    "        number of output channels.  Note that C_I should = C.\n",
    "    Returns\n",
    "    -------\n",
    "    result : np.ndarray\n",
    "        Convolved result.\n",
    "    \"\"\"\n",
    "    g = tf.Graph()\n",
    "    with tf.Session(graph=g):\n",
    "        convolved = tf.nn.conv2d(img, kernel, strides=[1, 1, 1, 1], padding='SAME')\n",
    "        res = convolved.eval()\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute a 2D Gabor Kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gabor(ksize=32):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    ksize : int, optional\n",
    "        Size of kernel.\n",
    "    Returns\n",
    "    -------\n",
    "    gabor : np.ndarray\n",
    "        Gabor kernel with ksize x ksize dimensions.\n",
    "    \"\"\"\n",
    "    g = tf.Graph()\n",
    "    with tf.Session(graph=g):\n",
    "        z_2d = gauss2d(0.0, 1.0, ksize)\n",
    "        ones = tf.ones((1, ksize))\n",
    "        ys = tf.sin(tf.linspace(-3.0, 3.0, ksize))\n",
    "        ys = tf.reshape(ys, [ksize, 1])\n",
    "        wave = tf.matmul(ys, ones)\n",
    "        gabor = tf.multiply(wave, z_2d)\n",
    "        return gabor.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Help utility to check the job assignments for submission and packages them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_submission(filename, file_list, optional_file_list=()):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    filename : str\n",
    "        Output zip file name\n",
    "    file_list : tuple\n",
    "        Tuple of files to include\n",
    "    \"\"\"\n",
    "    # check each file exists\n",
    "    for part_i, file_i in enumerate(file_list):\n",
    "        if not os.path.exists(file_i):\n",
    "            print('\\nYou are missing the file {}.  '.format(file_i) +\n",
    "                  'It does not look like you have completed Part {}.'.format(\n",
    "                part_i + 1))\n",
    "\n",
    "    def zipdir(path, zf):\n",
    "        for root, dirs, files in os.walk(path):\n",
    "            for file in files:\n",
    "                # make sure the files are part of the necessary file list\n",
    "                if file.endswith(file_list) or file.endswith(optional_file_list):\n",
    "                    zf.write(os.path.join(root, file))\n",
    "\n",
    "    # create a zip file with the necessary files\n",
    "    zipf = zipfile.ZipFile(filename, 'w', zipfile.ZIP_DEFLATED)\n",
    "    zipdir('.', zipf)\n",
    "    zipf.close()\n",
    "    print('Your assignment zip file has been created!')\n",
    "    print('Now submit the file:\\n{}\\nto Kadenze for grading!'.format(\n",
    "        os.path.abspath(filename)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize the image range for visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(a, s=0.1):\n",
    "    return np.uint8(np.clip(\n",
    "        (a - a.mean()) / max(a.std(), 1e-4) * s + 0.5,\n",
    "        0, 1) * 255)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use helper function to create a initialized weight variable with a normal distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "def weight_variable(shape, **kwargs):\n",
    "    '''\n",
    "    Parameters\n",
    "    ----------\n",
    "    shape : list\n",
    "        Size of weight variable\n",
    "    '''\n",
    "    if isinstance(shape, list):\n",
    "        initial = tf.random_normal(tf.stack(shape), mean=0.0, stddev=0.01)\n",
    "        initial.set_shape(shape)\n",
    "    else:\n",
    "        initial = tf.random_normal(shape, mean=0.0, stddev=0.01)\n",
    "    return tf.Variable(initial, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use helper function to create a initialized  bias variable with a constant value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "def bias_variable(shape, **kwargs):\n",
    "    '''\n",
    "    Parameters\n",
    "    ----------\n",
    "    shape : list\n",
    "        Size of weight variable\n",
    "    '''\n",
    "    if isinstance(shape, list):\n",
    "        initial = tf.random_normal(tf.stack(shape), mean=0.0, stddev=0.01)\n",
    "        initial.set_shape(shape)\n",
    "    else:\n",
    "        initial = tf.random_normal(shape, mean=0.0, stddev=0.01)\n",
    "    return tf.Variable(initial, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binary Cross Entropy measures cross entropy of a binary variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_cross_entropy(z, x):\n",
    "    \"\"\"\n",
    "    loss(x, z) = - sum_i (x[i] * log(z[i]) + (1 - x[i]) * log(1 - z[i]))\n",
    "    Parameters\n",
    "    ----------\n",
    "    z : tf.Tensor\n",
    "        A `Tensor` of the same type and shape as `x`.\n",
    "    x : tf.Tensor\n",
    "        A `Tensor` of type `float32` or `float64`.\n",
    "    \"\"\"\n",
    "    eps = 1e-12\n",
    "    return (-(x * tf.log(z + eps) +\n",
    "              (1. - x) * tf.log(1. - z + eps)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Use helper for creating a two-dimention convolution operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv2d(x, n_output,\n",
    "           k_h=5, k_w=5, d_h=2, d_w=2,\n",
    "           padding='SAME', name='conv2d', reuse=None):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    x : tf.Tensor\n",
    "        Input tensor to convolve.\n",
    "    n_output : int\n",
    "        Number of filters.\n",
    "    k_h : int, optional\n",
    "        Kernel height\n",
    "    k_w : int, optional\n",
    "        Kernel width\n",
    "    d_h : int, optional\n",
    "        Height stride\n",
    "    d_w : int, optional\n",
    "        Width stride\n",
    "    padding : str, optional\n",
    "        Padding type: \"SAME\" or \"VALID\"\n",
    "    name : str, optional\n",
    "        Variable scope\n",
    "    Returns\n",
    "    -------\n",
    "    op : tf.Tensor\n",
    "        Output of convolution\n",
    "    \"\"\"\n",
    "    with tf.variable_scope(name or 'conv2d', reuse=reuse):\n",
    "        W = tf.get_variable(\n",
    "            name='W',\n",
    "            shape=[k_h, k_w, x.get_shape()[-1], n_output],\n",
    "            initializer=tf.contrib.layers.xavier_initializer_conv2d())\n",
    "\n",
    "        conv = tf.nn.conv2d(\n",
    "            name='conv',\n",
    "            input=x,\n",
    "            filter=W,\n",
    "            strides=[1, d_h, d_w, 1],\n",
    "            padding=padding)\n",
    "\n",
    "        b = tf.get_variable(\n",
    "            name='b',\n",
    "            shape=[n_output],\n",
    "            initializer=tf.constant_initializer(0.0))\n",
    "\n",
    "        h = tf.nn.bias_add(\n",
    "            name='h',\n",
    "            value=conv,\n",
    "            bias=b)\n",
    "\n",
    "    return h, W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use deconvolution helper to deconvolute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deconv2d(x, n_output_h, n_output_w, n_output_ch, n_input_ch=None,\n",
    "             k_h=5, k_w=5, d_h=2, d_w=2,\n",
    "             padding='SAME', name='deconv2d', reuse=None):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    x : tf.Tensor\n",
    "        Input tensor to convolve.\n",
    "    n_output_h : int\n",
    "        Height of output\n",
    "    n_output_w : int\n",
    "        Width of output\n",
    "    n_output_ch : int\n",
    "        Number of filters.\n",
    "    k_h : int, optional\n",
    "        Kernel height\n",
    "    k_w : int, optional\n",
    "        Kernel width\n",
    "    d_h : int, optional\n",
    "        Height stride\n",
    "    d_w : int, optional\n",
    "        Width stride\n",
    "    padding : str, optional\n",
    "        Padding type: \"SAME\" or \"VALID\"\n",
    "    name : str, optional\n",
    "        Variable scope\n",
    "    Returns\n",
    "    -------\n",
    "    op : tf.Tensor\n",
    "        Output of deconvolution\n",
    "    \"\"\"\n",
    "    with tf.variable_scope(name or 'deconv2d', reuse=reuse):\n",
    "        W = tf.get_variable(\n",
    "            name='W',\n",
    "            shape=[k_h, k_w, n_output_ch, n_input_ch or x.get_shape()[-1]],\n",
    "            initializer=tf.contrib.layers.xavier_initializer_conv2d())\n",
    "\n",
    "        conv = tf.nn.conv2d_transpose(\n",
    "            name='conv_t',\n",
    "            value=x,\n",
    "            filter=W,\n",
    "            output_shape=tf.stack(\n",
    "                [tf.shape(x)[0], n_output_h, n_output_w, n_output_ch]),\n",
    "            strides=[1, d_h, d_w, 1],\n",
    "            padding=padding)\n",
    "\n",
    "        conv.set_shape([None, n_output_h, n_output_w, n_output_ch])\n",
    "\n",
    "        b = tf.get_variable(\n",
    "            name='b',\n",
    "            shape=[n_output_ch],\n",
    "            initializer=tf.constant_initializer(0.0))\n",
    "\n",
    "        h = tf.nn.bias_add(name='h', value=conv, bias=b)\n",
    "\n",
    "    return h, W\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use rectifire to activate functions(defined as the positive part of its argument)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lrelu(features, leak=0.2):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    features : tf.Tensor\n",
    "        Input to apply leaky rectifier to.\n",
    "    leak : float, optional\n",
    "        Percentage of leak.\n",
    "    Returns\n",
    "    -------\n",
    "    op : tf.Tensor\n",
    "        Resulting output of applying leaky rectifier activation.\n",
    "    \"\"\"\n",
    "    f1 = 0.5 * (1 + leak)\n",
    "    f2 = 0.5 * (1 - leak)\n",
    "    return f1 * features + f2 * abs(features)\n",
    "\n",
    "\n",
    "def linear(x, n_output, name=None, activation=None, reuse=None):\n",
    "    \"\"\"Fully connected layer.\n",
    "    Parameters\n",
    "    ----------\n",
    "    x : tf.Tensor\n",
    "        Input tensor to connect\n",
    "    n_output : int\n",
    "        Number of output neurons\n",
    "    name : None, optional\n",
    "        Scope to apply\n",
    "    Returns\n",
    "    -------\n",
    "    h, W : tf.Tensor, tf.Tensor\n",
    "        Output of fully connected layer and the weight matrix\n",
    "    \"\"\"\n",
    "    if len(x.get_shape()) != 2:\n",
    "        x = flatten(x, reuse=reuse)\n",
    "\n",
    "    n_input = x.get_shape().as_list()[1]\n",
    "\n",
    "    with tf.variable_scope(name or \"fc\", reuse=reuse):\n",
    "        W = tf.get_variable(\n",
    "            name='W',\n",
    "            shape=[n_input, n_output],\n",
    "            dtype=tf.float32,\n",
    "            initializer=tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "        b = tf.get_variable(\n",
    "            name='b',\n",
    "            shape=[n_output],\n",
    "            dtype=tf.float32,\n",
    "            initializer=tf.constant_initializer(0.0))\n",
    "\n",
    "        h = tf.nn.bias_add(\n",
    "            name='h',\n",
    "            value=tf.matmul(x, W),\n",
    "            bias=b)\n",
    "\n",
    "        if activation:\n",
    "            h = activation(h)\n",
    "\n",
    "        return h, W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flatten the tensor to 2 dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(x, name=None, reuse=None):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    x : tf.Tensor\n",
    "        Input tensor to flatten.\n",
    "    name : None, optional\n",
    "        Variable scope for flatten operations\n",
    "    Returns\n",
    "    -------\n",
    "    flattened : tf.Tensor\n",
    "        Flattened tensor.\n",
    "    \"\"\"\n",
    "    with tf.variable_scope('flatten'):\n",
    "        dims = x.get_shape().as_list()\n",
    "        if len(dims) == 4:\n",
    "            flattened = tf.reshape(\n",
    "                x,\n",
    "                shape=[-1, dims[1] * dims[2] * dims[3]])\n",
    "        elif len(dims) == 2 or len(dims) == 1:\n",
    "            flattened = x\n",
    "        else:\n",
    "            raise ValueError('Expected n dimensions of 1, 2 or 4.  Found:',\n",
    "                             len(dims))\n",
    "\n",
    "        return flattened"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert 2 dim Tensor to a 4 dim Tensor ready for convolution. Performs the opposite of flatten(x).  If the tensor is already 4-D, this returns the same as the input, leaving it unchanged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_tensor(x):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    x : tf.Tesnor\n",
    "        Input 2-D tensor.  If 4-D already, left unchanged.\n",
    "    Returns\n",
    "    -------\n",
    "    x : tf.Tensor\n",
    "        4-D representation of the input.\n",
    "    Raises\n",
    "    ------\n",
    "    ValueError\n",
    "        If the tensor is not 2D or already 4D.\n",
    "    \"\"\"\n",
    "    if len(x.get_shape()) == 2:\n",
    "        n_input = x.get_shape().as_list()[1]\n",
    "        x_dim = np.sqrt(n_input)\n",
    "        if x_dim == int(x_dim):\n",
    "            x_dim = int(x_dim)\n",
    "            x_tensor = tf.reshape(\n",
    "                x, [-1, x_dim, x_dim, 1], name='reshape')\n",
    "        elif np.sqrt(n_input / 3) == int(np.sqrt(n_input / 3)):\n",
    "            x_dim = int(np.sqrt(n_input / 3))\n",
    "            x_tensor = tf.reshape(\n",
    "                x, [-1, x_dim, x_dim, 3], name='reshape')\n",
    "        else:\n",
    "            x_tensor = tf.reshape(\n",
    "                x, [-1, 1, 1, n_input], name='reshape')\n",
    "    elif len(x.get_shape()) == 4:\n",
    "        x_tensor = x\n",
    "    else:\n",
    "        raise ValueError('Unsupported input dimensions')\n",
    "    return x_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reference:\n",
    "https://stackoverflow.com/questions/40615034/understanding-scipy-deconvolve   \n",
    "https://www.quora.com/What-are-the-advantages-of-using-Leaky-Rectified-Linear-Units-Leaky-ReLU-over-normal-ReLU-in-deep-learning\n",
    "https://en.wikipedia.org/wiki/Rectifier_(neural_networks)\n",
    "https://rdipietro.github.io/friendly-intro-to-cross-entropy-loss/\n",
    "https://arxiv.org/pdf/1704.05231.pdf\n",
    "http://www.stat.wisc.edu/~mchung/teaching/MIA/reading/diffusion.gaussian.kernel.pdf.pdf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
